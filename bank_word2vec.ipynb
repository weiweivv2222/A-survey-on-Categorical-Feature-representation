{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9549a6-6582-4df3-825b-509129af9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle result_pkl = {'accuracies': accuracies, 'precisions': precisions, 'recalls': recalls, 'f1s': f1s, 'rocs': roc_aucs}\n",
    "# Save to a pickle file\n",
    "with open('results.pkl', 'wb') as f:     pickle.dump(result_pkl, f)\n",
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4214ff27-0404-4c2c-91d4-28dcd3d3bae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.pkl', 'rb') as f:     loaded_result_pkl = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee136e1f-3fe9-4391-aa8c-434689de57ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\P70071469\\AppData\\Local\\anaconda3\\envs\\SLR\\lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "C:\\Users\\P70071469\\AppData\\Local\\anaconda3\\envs\\SLR\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.\n",
      "  \"class\": algorithms.Blowfish,\n",
      "C:\\Users\\P70071469\\AppData\\Local\\anaconda3\\envs\\SLR\\lib\\site-packages\\paramiko\\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LR\n",
      "Accuracy: 0.9075 (95% CI: 0.9047 - 0.9103)\n",
      "Precision: 0.6643 (95% CI: 0.6419 - 0.6868)\n",
      "Recall: 0.3621 (95% CI: 0.3311 - 0.3931)\n",
      "F1 Score: 0.4683 (95% CI: 0.4419 - 0.4947)\n",
      "ROC AUC: 0.9142 (95% CI: 0.9088 - 0.9195)\n",
      "Average Computation Time per Fold: 1.6623 seconds\n",
      "\n",
      "Classifier: DT\n",
      "Accuracy: 0.9061 (95% CI: 0.9024 - 0.9098)\n",
      "Precision: 0.6509 (95% CI: 0.6272 - 0.6745)\n",
      "Recall: 0.3591 (95% CI: 0.3263 - 0.3918)\n",
      "F1 Score: 0.4625 (95% CI: 0.4317 - 0.4933)\n",
      "ROC AUC: 0.8515 (95% CI: 0.8411 - 0.8619)\n",
      "Average Computation Time per Fold: 0.8710 seconds\n",
      "\n",
      "Classifier: RF\n",
      "Accuracy: 0.8989 (95% CI: 0.8949 - 0.9028)\n",
      "Precision: 0.6871 (95% CI: 0.6493 - 0.7248)\n",
      "Recall: 0.1860 (95% CI: 0.1460 - 0.2260)\n",
      "F1 Score: 0.2920 (95% CI: 0.2402 - 0.3438)\n",
      "ROC AUC: 0.8813 (95% CI: 0.8718 - 0.8908)\n",
      "Average Computation Time per Fold: 9.0679 seconds\n",
      "\n",
      "Classifier: KNN\n",
      "Accuracy: 0.8869 (95% CI: 0.8834 - 0.8903)\n",
      "Precision: 0.4942 (95% CI: 0.4644 - 0.5241)\n",
      "Recall: 0.2446 (95% CI: 0.2063 - 0.2829)\n",
      "F1 Score: 0.3269 (95% CI: 0.2867 - 0.3672)\n",
      "ROC AUC: 0.6955 (95% CI: 0.6788 - 0.7122)\n",
      "Average Computation Time per Fold: 0.1138 seconds\n",
      "\n",
      "Classifier: XGB\n",
      "Accuracy: 0.9089 (95% CI: 0.9048 - 0.9130)\n",
      "Precision: 0.6233 (95% CI: 0.5983 - 0.6484)\n",
      "Recall: 0.4849 (95% CI: 0.4540 - 0.5158)\n",
      "F1 Score: 0.5452 (95% CI: 0.5210 - 0.5694)\n",
      "ROC AUC: 0.9261 (95% CI: 0.9218 - 0.9304)\n",
      "Average Computation Time per Fold: 3.7682 seconds\n",
      "\n",
      "Classifier: MLP\n",
      "Accuracy: 0.8953 (95% CI: 0.8933 - 0.8973)\n",
      "Precision: 0.5443 (95% CI: 0.5308 - 0.5579)\n",
      "Recall: 0.4375 (95% CI: 0.4119 - 0.4631)\n",
      "F1 Score: 0.4848 (95% CI: 0.4711 - 0.4984)\n",
      "ROC AUC: 0.8945 (95% CI: 0.8880 - 0.9009)\n",
      "Average Computation Time per Fold: 64.6858 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import config_cat_embedding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, confusion_matrix, classification_report)\n",
    "from tqdm.notebook import tqdm\n",
    "from data_prep import bank_data_prep\n",
    "from embedding_helper import create_network\n",
    "from scipy import stats  # For confidence intervals\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load and preprocess data\n",
    "data_path = config_cat_embedding.paths['data']\n",
    "data_path_out = config_cat_embedding.paths['data_output']\n",
    "bank_data = pd.read_csv(data_path + 'bank-additional-full.csv', sep=';')\n",
    "\n",
    "df_bank, cat_cols = bank_data_prep(bank_data)\n",
    "\n",
    "X = df_bank.iloc[:, :-1]\n",
    "y = df_bank.y\n",
    "\n",
    "\n",
    "# Define the classifiers\n",
    "seed = 42\n",
    "\n",
    "models = [\n",
    "    ('LR', LogisticRegression(solver='lbfgs', random_state=seed, max_iter=1000)),\n",
    "    ('DT', DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=seed)),\n",
    "    ('RF', RandomForestClassifier(n_estimators=200, max_depth=5, random_state=seed, min_samples_leaf=3)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=3)),\n",
    "    ('XGB', XGBClassifier(eval_metric='logloss', random_state=seed)),\n",
    "    # ('SVM', SVC(gamma='scale', random_state=seed, probability=True)),\n",
    "    ('MLP', KerasClassifier(\n",
    "        model=create_network,\n",
    "        epochs=100, batch_size=100, verbose=0, random_state=seed))\n",
    "]\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)  # You can adjust n_splits as needed\n",
    "\n",
    "# Function to calculate confidence intervals\n",
    "def confidence_interval(data, confidence=0.95):\n",
    "    n = len(data)\n",
    "    m = np.mean(data)\n",
    "    std_err = stats.sem(data)\n",
    "    h = std_err * stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "    return m, m - h, m + h\n",
    "\n",
    "# Move the embedding function outside the loop\n",
    "def get_word2vec_embeddings(df, cat_cols, model, dimpool):\n",
    "    elements = []\n",
    "    for _, row in df.iterrows():\n",
    "        categorical_embeddings = []\n",
    "        for col in cat_cols:\n",
    "            try:\n",
    "                categorical_embeddings.append(model.wv[row[col]])\n",
    "            except KeyError:\n",
    "                categorical_embeddings.append(np.zeros((dimpool,)))\n",
    "        elements.append(np.array(categorical_embeddings))\n",
    "    reshaped_x = np.reshape(elements, (len(elements), len(cat_cols) * dimpool))\n",
    "    return reshaped_x\n",
    "\n",
    "# Main loop over models\n",
    "for name, classifier in models:\n",
    "    print(f\"Classifier: {name}\")\n",
    "    # Lists to store metrics for each fold\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    roc_aucs = []\n",
    "    computation_times = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        # Split data into training and test sets for this fold\n",
    "        X_train_fold = X.iloc[train_index].copy()\n",
    "        X_test_fold = X.iloc[test_index].copy()\n",
    "        y_train_fold = y.iloc[train_index].reset_index(drop=True)\n",
    "        y_test_fold = y.iloc[test_index].reset_index(drop=True)\n",
    "\n",
    "        # Combine categorical columns into a single string for Word2Vec\n",
    "        X_train_fold['stringcat'] = X_train_fold[cat_cols].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "        # Train Word2Vec model on training data\n",
    "        dimpool = 50  # Embedding dimension\n",
    "        word2vec_model = Word2Vec(sentences=X_train_fold['stringcat'].str.split(\" \"), vector_size=dimpool,\n",
    "                                  window=2, min_count=1, workers=1, seed=42)\n",
    "\n",
    "        # Generate embeddings for training data\n",
    "        X_train_emb = get_word2vec_embeddings(X_train_fold, cat_cols, word2vec_model, dimpool)\n",
    "\n",
    "        # Handle numerical features\n",
    "        numerical_cols = X_train_fold.select_dtypes(exclude='object').columns.tolist()\n",
    "        X_train_num = X_train_fold[numerical_cols].reset_index(drop=True)\n",
    "\n",
    "        # Create DataFrame for embeddings with string column names\n",
    "        emb_col_names = [f'emb_{i}' for i in range(X_train_emb.shape[1])]\n",
    "        X_train_emb_df = pd.DataFrame(X_train_emb, columns=emb_col_names)\n",
    "\n",
    "        # Concatenate numerical features and embeddings\n",
    "        X_train_combined = pd.concat([X_train_num, X_train_emb_df], axis=1)\n",
    "\n",
    "        # Ensure all column names are strings\n",
    "        X_train_combined.columns = X_train_combined.columns.astype(str)\n",
    "\n",
    "        # Generate embeddings for test data\n",
    "        X_test_emb = get_word2vec_embeddings(X_test_fold, cat_cols, word2vec_model, dimpool)\n",
    "        X_test_num = X_test_fold[numerical_cols].reset_index(drop=True)\n",
    "        X_test_emb_df = pd.DataFrame(X_test_emb, columns=emb_col_names)\n",
    "        X_test_combined = pd.concat([X_test_num, X_test_emb_df], axis=1)\n",
    "        X_test_combined.columns = X_test_combined.columns.astype(str)\n",
    "\n",
    "        # Standard scaling\n",
    "        stc = StandardScaler()\n",
    "        X_train_scaled = stc.fit_transform(X_train_combined)\n",
    "        X_test_scaled = stc.transform(X_test_combined)\n",
    "\n",
    "        # Update number_of_features for MLP\n",
    "        number_of_features = X_train_scaled.shape[1]\n",
    "        if name == 'MLP':\n",
    "            classifier.set_params(model__number_of_features=number_of_features)\n",
    "\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Fit the model\n",
    "        classifier.fit(X_train_scaled, y_train_fold)\n",
    "\n",
    "        # End timing\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        computation_times.append(elapsed_time)\n",
    "\n",
    "        # Predict on test data\n",
    "        y_pred_fold = classifier.predict(X_test_scaled)\n",
    "        if hasattr(classifier, \"predict_proba\"):\n",
    "            y_pred_prob_fold = classifier.predict_proba(X_test_scaled)[:, 1]\n",
    "        else:\n",
    "            y_pred_scores = classifier.decision_function(X_test_scaled)\n",
    "            y_pred_prob_fold = (y_pred_scores - y_pred_scores.min()) / (y_pred_scores.max() - y_pred_scores.min())\n",
    "\n",
    "        # Collect performance metrics\n",
    "        accuracies.append(accuracy_score(y_test_fold, y_pred_fold))\n",
    "        precisions.append(precision_score(y_test_fold, y_pred_fold, zero_division=0))\n",
    "        recalls.append(recall_score(y_test_fold, y_pred_fold))\n",
    "        f1s.append(f1_score(y_test_fold, y_pred_fold))\n",
    "        roc_aucs.append(roc_auc_score(y_test_fold, y_pred_prob_fold))\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    # Calculate mean and confidence intervals\n",
    "    acc_mean, acc_ci_lower, acc_ci_upper = confidence_interval(accuracies)\n",
    "    prec_mean, prec_ci_lower, prec_ci_upper = confidence_interval(precisions)\n",
    "    rec_mean, rec_ci_lower, rec_ci_upper = confidence_interval(recalls)\n",
    "    f1_mean, f1_ci_lower, f1_ci_upper = confidence_interval(f1s)\n",
    "    roc_mean, roc_ci_lower, roc_ci_upper = confidence_interval(roc_aucs)\n",
    "    time_mean = np.mean(computation_times)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Accuracy: {acc_mean:.4f} (95% CI: {acc_ci_lower:.4f} - {acc_ci_upper:.4f})\")\n",
    "    print(f\"Precision: {prec_mean:.4f} (95% CI: {prec_ci_lower:.4f} - {prec_ci_upper:.4f})\")\n",
    "    print(f\"Recall: {rec_mean:.4f} (95% CI: {rec_ci_lower:.4f} - {rec_ci_upper:.4f})\")\n",
    "    print(f\"F1 Score: {f1_mean:.4f} (95% CI: {f1_ci_lower:.4f} - {f1_ci_upper:.4f})\")\n",
    "    print(f\"ROC AUC: {roc_mean:.4f} (95% CI: {roc_ci_lower:.4f} - {roc_ci_upper:.4f})\")\n",
    "    print(f\"Average Computation Time per Fold: {time_mean:.4f} seconds\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4642394c-7b0b-4e97-9bf2-b6ed812e7d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LR\n",
      "Accuracy: 0.9075 (95% CI: 0.9047 - 0.9103)\n",
      "Precision: 0.6644 (95% CI: 0.6404 - 0.6884)\n",
      "Recall: 0.3621 (95% CI: 0.3319 - 0.3923)\n",
      "F1 Score: 0.4683 (95% CI: 0.4424 - 0.4942)\n",
      "ROC AUC: 0.9141 (95% CI: 0.9088 - 0.9194)\n",
      "Average Computation Time per Fold: 1.2059 seconds\n",
      "\n",
      "Classifier: DT\n",
      "Accuracy: 0.9061 (95% CI: 0.9024 - 0.9098)\n",
      "Precision: 0.6509 (95% CI: 0.6272 - 0.6745)\n",
      "Recall: 0.3591 (95% CI: 0.3263 - 0.3918)\n",
      "F1 Score: 0.4625 (95% CI: 0.4317 - 0.4933)\n",
      "ROC AUC: 0.8515 (95% CI: 0.8411 - 0.8619)\n",
      "Average Computation Time per Fold: 0.5338 seconds\n",
      "\n",
      "Classifier: RF\n",
      "Accuracy: 0.8991 (95% CI: 0.8950 - 0.9031)\n",
      "Precision: 0.6913 (95% CI: 0.6518 - 0.7307)\n",
      "Recall: 0.1864 (95% CI: 0.1479 - 0.2249)\n",
      "F1 Score: 0.2930 (95% CI: 0.2427 - 0.3433)\n",
      "ROC AUC: 0.8933 (95% CI: 0.8863 - 0.9003)\n",
      "Average Computation Time per Fold: 7.4279 seconds\n",
      "\n",
      "Classifier: KNN\n",
      "Accuracy: 0.8884 (95% CI: 0.8855 - 0.8913)\n",
      "Precision: 0.5088 (95% CI: 0.4841 - 0.5335)\n",
      "Recall: 0.2504 (95% CI: 0.2191 - 0.2817)\n",
      "F1 Score: 0.3354 (95% CI: 0.3025 - 0.3683)\n",
      "ROC AUC: 0.6971 (95% CI: 0.6754 - 0.7188)\n",
      "Average Computation Time per Fold: 0.0654 seconds\n",
      "\n",
      "Classifier: XGB\n",
      "Accuracy: 0.9091 (95% CI: 0.9058 - 0.9124)\n",
      "Precision: 0.6234 (95% CI: 0.6029 - 0.6440)\n",
      "Recall: 0.4886 (95% CI: 0.4608 - 0.5164)\n",
      "F1 Score: 0.5476 (95% CI: 0.5278 - 0.5673)\n",
      "ROC AUC: 0.9270 (95% CI: 0.9230 - 0.9310)\n",
      "Average Computation Time per Fold: 1.8565 seconds\n",
      "\n",
      "Classifier: MLP\n",
      "Accuracy: 0.8947 (95% CI: 0.8922 - 0.8972)\n",
      "Precision: 0.5442 (95% CI: 0.5244 - 0.5641)\n",
      "Recall: 0.4106 (95% CI: 0.3600 - 0.4611)\n",
      "F1 Score: 0.4668 (95% CI: 0.4381 - 0.4955)\n",
      "ROC AUC: 0.8983 (95% CI: 0.8920 - 0.9045)\n",
      "Average Computation Time per Fold: 60.1206 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import config_cat_embedding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, confusion_matrix, classification_report)\n",
    "from tqdm.notebook import tqdm\n",
    "from data_prep import bank_data_prep\n",
    "from embedding_helper import create_network\n",
    "from scipy import stats  # For confidence intervals\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load and preprocess data\n",
    "data_path = config_cat_embedding.paths['data']\n",
    "data_path_out = config_cat_embedding.paths['data_output']\n",
    "bank_data = pd.read_csv(data_path + 'bank-additional-full.csv', sep=';')\n",
    "\n",
    "df_bank, cat_cols = bank_data_prep(bank_data)\n",
    "\n",
    "X = df_bank.iloc[:, :-1]\n",
    "y = df_bank.y\n",
    "\n",
    "\n",
    "# Define the classifiers\n",
    "seed = 42\n",
    "\n",
    "models = [\n",
    "    ('LR', LogisticRegression(solver='lbfgs', random_state=seed, max_iter=1000)),\n",
    "    ('DT', DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=seed)),\n",
    "    ('RF', RandomForestClassifier(n_estimators=200, max_depth=5, random_state=seed, min_samples_leaf=3)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=3)),\n",
    "    ('XGB', XGBClassifier(eval_metric='logloss', random_state=seed)),\n",
    "    # ('SVM', SVC(gamma='scale', random_state=seed, probability=True)),\n",
    "    ('MLP', KerasClassifier(\n",
    "        model=create_network,\n",
    "        epochs=100, batch_size=100, verbose=0, random_state=seed))\n",
    "]\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)  # You can adjust n_splits as needed\n",
    "\n",
    "# Function to calculate confidence intervals\n",
    "def confidence_interval(data, confidence=0.95):\n",
    "    n = len(data)\n",
    "    m = np.mean(data)\n",
    "    std_err = stats.sem(data)\n",
    "    h = std_err * stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "    return m, m - h, m + h\n",
    "\n",
    "# Move the embedding function outside the loop\n",
    "def get_word2vec_embeddings(df, cat_cols, model, dimpool):\n",
    "    elements = []\n",
    "    for _, row in df.iterrows():\n",
    "        categorical_embeddings = []\n",
    "        for col in cat_cols:\n",
    "            try:\n",
    "                categorical_embeddings.append(model.wv[row[col]])\n",
    "            except KeyError:\n",
    "                categorical_embeddings.append(np.zeros((dimpool,)))\n",
    "        elements.append(np.array(categorical_embeddings))\n",
    "    reshaped_x = np.reshape(elements, (len(elements), len(cat_cols) * dimpool))\n",
    "    return reshaped_x\n",
    "\n",
    "# Main loop over models\n",
    "for name, classifier in models:\n",
    "    print(f\"Classifier: {name}\")\n",
    "    # Lists to store metrics for each fold\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    roc_aucs = []\n",
    "    computation_times = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        # Split data into training and test sets for this fold\n",
    "        X_train_fold = X.iloc[train_index].copy()\n",
    "        X_test_fold = X.iloc[test_index].copy()\n",
    "        y_train_fold = y.iloc[train_index].reset_index(drop=True)\n",
    "        y_test_fold = y.iloc[test_index].reset_index(drop=True)\n",
    "\n",
    "        # Combine categorical columns into a single string for Word2Vec\n",
    "        X_train_fold['stringcat'] = X_train_fold[cat_cols].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "        # Train Word2Vec model on training data\n",
    "        dimpool = 30  # Embedding dimension\n",
    "        word2vec_model = Word2Vec(sentences=X_train_fold['stringcat'].str.split(\" \"), vector_size=dimpool,\n",
    "                                  window=2, min_count=1, workers=1, seed=42)\n",
    "\n",
    "        # Generate embeddings for training data\n",
    "        X_train_emb = get_word2vec_embeddings(X_train_fold, cat_cols, word2vec_model, dimpool)\n",
    "\n",
    "        # Handle numerical features\n",
    "        numerical_cols = X_train_fold.select_dtypes(exclude='object').columns.tolist()\n",
    "        X_train_num = X_train_fold[numerical_cols].reset_index(drop=True)\n",
    "\n",
    "        # Create DataFrame for embeddings with string column names\n",
    "        emb_col_names = [f'emb_{i}' for i in range(X_train_emb.shape[1])]\n",
    "        X_train_emb_df = pd.DataFrame(X_train_emb, columns=emb_col_names)\n",
    "\n",
    "        # Concatenate numerical features and embeddings\n",
    "        X_train_combined = pd.concat([X_train_num, X_train_emb_df], axis=1)\n",
    "\n",
    "        # Ensure all column names are strings\n",
    "        X_train_combined.columns = X_train_combined.columns.astype(str)\n",
    "\n",
    "        # Generate embeddings for test data\n",
    "        X_test_emb = get_word2vec_embeddings(X_test_fold, cat_cols, word2vec_model, dimpool)\n",
    "        X_test_num = X_test_fold[numerical_cols].reset_index(drop=True)\n",
    "        X_test_emb_df = pd.DataFrame(X_test_emb, columns=emb_col_names)\n",
    "        X_test_combined = pd.concat([X_test_num, X_test_emb_df], axis=1)\n",
    "        X_test_combined.columns = X_test_combined.columns.astype(str)\n",
    "\n",
    "        # Standard scaling\n",
    "        stc = StandardScaler()\n",
    "        X_train_scaled = stc.fit_transform(X_train_combined)\n",
    "        X_test_scaled = stc.transform(X_test_combined)\n",
    "\n",
    "        # Update number_of_features for MLP\n",
    "        number_of_features = X_train_scaled.shape[1]\n",
    "        if name == 'MLP':\n",
    "            classifier.set_params(model__number_of_features=number_of_features)\n",
    "\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Fit the model\n",
    "        classifier.fit(X_train_scaled, y_train_fold)\n",
    "\n",
    "        # End timing\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        computation_times.append(elapsed_time)\n",
    "\n",
    "        # Predict on test data\n",
    "        y_pred_fold = classifier.predict(X_test_scaled)\n",
    "        if hasattr(classifier, \"predict_proba\"):\n",
    "            y_pred_prob_fold = classifier.predict_proba(X_test_scaled)[:, 1]\n",
    "        else:\n",
    "            y_pred_scores = classifier.decision_function(X_test_scaled)\n",
    "            y_pred_prob_fold = (y_pred_scores - y_pred_scores.min()) / (y_pred_scores.max() - y_pred_scores.min())\n",
    "\n",
    "        # Collect performance metrics\n",
    "        accuracies.append(accuracy_score(y_test_fold, y_pred_fold))\n",
    "        precisions.append(precision_score(y_test_fold, y_pred_fold, zero_division=0))\n",
    "        recalls.append(recall_score(y_test_fold, y_pred_fold))\n",
    "        f1s.append(f1_score(y_test_fold, y_pred_fold))\n",
    "        roc_aucs.append(roc_auc_score(y_test_fold, y_pred_prob_fold))\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    # Calculate mean and confidence intervals\n",
    "    acc_mean, acc_ci_lower, acc_ci_upper = confidence_interval(accuracies)\n",
    "    prec_mean, prec_ci_lower, prec_ci_upper = confidence_interval(precisions)\n",
    "    rec_mean, rec_ci_lower, rec_ci_upper = confidence_interval(recalls)\n",
    "    f1_mean, f1_ci_lower, f1_ci_upper = confidence_interval(f1s)\n",
    "    roc_mean, roc_ci_lower, roc_ci_upper = confidence_interval(roc_aucs)\n",
    "    time_mean = np.mean(computation_times)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Accuracy: {acc_mean:.4f} (95% CI: {acc_ci_lower:.4f} - {acc_ci_upper:.4f})\")\n",
    "    print(f\"Precision: {prec_mean:.4f} (95% CI: {prec_ci_lower:.4f} - {prec_ci_upper:.4f})\")\n",
    "    print(f\"Recall: {rec_mean:.4f} (95% CI: {rec_ci_lower:.4f} - {rec_ci_upper:.4f})\")\n",
    "    print(f\"F1 Score: {f1_mean:.4f} (95% CI: {f1_ci_lower:.4f} - {f1_ci_upper:.4f})\")\n",
    "    print(f\"ROC AUC: {roc_mean:.4f} (95% CI: {roc_ci_lower:.4f} - {roc_ci_upper:.4f})\")\n",
    "    print(f\"Average Computation Time per Fold: {time_mean:.4f} seconds\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52f595dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xg16060\\AppData\\Local\\Temp\\1\\ipykernel_34020\\2530986181.py:43: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for row in tqdm(X_train.iterrows()):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5545aeb4d9b4ded9929281087dee46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xg16060\\AppData\\Local\\Temp\\1\\ipykernel_34020\\2530986181.py:65: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for row in tqdm(X_test.iterrows()):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9803764adde494c99657a23a2b493f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xg16060\\AppData\\Local\\Temp\\1\\ipykernel_34020\\2530986181.py:96: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  ('MLP', KerasClassifier(build_fn=create_network, number_of_features=my_data.shape[1], epochs=100, batch_size=100, verbose=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LR\n",
      "Computation Time: 1.226273536682129 seconds\n",
      "[[7123  171]\n",
      " [ 607  337]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9215    0.9766    0.9482      7294\n",
      "           1     0.6634    0.3570    0.4642       944\n",
      "\n",
      "    accuracy                         0.9056      8238\n",
      "   macro avg     0.7924    0.6668    0.7062      8238\n",
      "weighted avg     0.8919    0.9056    0.8928      8238\n",
      "\n",
      "0.9169146744712394\n",
      "\n",
      "Classifier: DT\n",
      "Computation Time: 0.2270958423614502 seconds\n",
      "[[7049  245]\n",
      " [ 570  374]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9252    0.9664    0.9453      7294\n",
      "           1     0.6042    0.3962    0.4786       944\n",
      "\n",
      "    accuracy                         0.9011      8238\n",
      "   macro avg     0.7647    0.6813    0.7120      8238\n",
      "weighted avg     0.8884    0.9011    0.8919      8238\n",
      "\n",
      "0.8489391385071546\n",
      "\n",
      "Classifier: RF\n",
      "Computation Time: 3.1036245822906494 seconds\n",
      "[[7212   82]\n",
      " [ 753  191]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9055    0.9888    0.9453      7294\n",
      "           1     0.6996    0.2023    0.3139       944\n",
      "\n",
      "    accuracy                         0.8986      8238\n",
      "   macro avg     0.8025    0.5955    0.6296      8238\n",
      "weighted avg     0.8819    0.8986    0.8729      8238\n",
      "\n",
      "0.9006846380586783\n",
      "\n",
      "Classifier: KNN\n",
      "Computation Time: 0.03699994087219238 seconds\n",
      "[[7081  213]\n",
      " [ 705  239]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9095    0.9708    0.9391      7294\n",
      "           1     0.5288    0.2532    0.3424       944\n",
      "\n",
      "    accuracy                         0.8886      8238\n",
      "   macro avg     0.7191    0.6120    0.6408      8238\n",
      "weighted avg     0.8658    0.8886    0.8707      8238\n",
      "\n",
      "0.6973200779140506\n",
      "\n",
      "Classifier: XGB\n",
      "Computation Time: 3.8426883220672607 seconds\n",
      "[[7006  288]\n",
      " [ 477  467]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9363    0.9605    0.9482      7294\n",
      "           1     0.6185    0.4947    0.5497       944\n",
      "\n",
      "    accuracy                         0.9071      8238\n",
      "   macro avg     0.7774    0.7276    0.7490      8238\n",
      "weighted avg     0.8998    0.9071    0.9026      8238\n",
      "\n",
      "0.9287879694478397\n",
      "\n",
      "Classifier: SVM\n",
      "Computation Time: 275.41855907440186 seconds\n",
      "[[7172  122]\n",
      " [ 700  244]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9111    0.9833    0.9458      7294\n",
      "           1     0.6667    0.2585    0.3725       944\n",
      "\n",
      "    accuracy                         0.9002      8238\n",
      "   macro avg     0.7889    0.6209    0.6592      8238\n",
      "weighted avg     0.8831    0.9002    0.8801      8238\n",
      "\n",
      "0.9150763281173753\n",
      "\n",
      "Classifier: MLP\n",
      "Computation Time: 19.805487632751465 seconds\n",
      "258/258 [==============================] - 0s 545us/step\n",
      "258/258 [==============================] - 0s 550us/step\n",
      "[[7072  222]\n",
      " [ 544  400]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9286    0.9696    0.9486      7294\n",
      "           1     0.6431    0.4237    0.5109       944\n",
      "\n",
      "    accuracy                         0.9070      8238\n",
      "   macro avg     0.7858    0.6966    0.7297      8238\n",
      "weighted avg     0.8959    0.9070    0.8985      8238\n",
      "\n",
      "0.9258637962244333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import config_cat_embedding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix,classification_report,auc, roc_auc_score\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from data_prep import bank_data_prep\n",
    "from embedding_helper import create_network\n",
    "# Set the random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "#%% load the data and completed the data pre-processing\n",
    "data_path = config_cat_embedding.paths['data']\n",
    "data_path_out = config_cat_embedding.paths['data_output']\n",
    "bank_data = pd.read_csv(data_path + 'bank-additional-full.csv', sep=';')\n",
    "\n",
    "dat_bank, cat_cols = bank_data_prep(bank_data)\n",
    "\n",
    "X = dat_bank.iloc[:, :-1]\n",
    "target = dat_bank.y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.2, random_state=1500)\n",
    "\n",
    "#Get categorical columns\n",
    "X_train['stringcat'] = X_train[cat_cols].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "#train word2vec model\n",
    "dimpool = 30\n",
    "model = Word2Vec(sentences=X_train['stringcat'].str.split(\" \"), vector_size=dimpool, window=2, min_count=1, workers=1,seed=42)\n",
    "model.save(\"word2vec.model.bank\")\n",
    "\n",
    "\n",
    "#model.wv['entrepreneur']\n",
    "elements = []\n",
    "for row in tqdm(X_train.iterrows()):\n",
    "    categorical_embeddings = []\n",
    "    for i in cat_cols:\n",
    "        #print(i,row[1][i])\n",
    "        try:\n",
    "            \n",
    "            categorical_embeddings.append(model.wv[row[1][i]])\n",
    "        except:\n",
    "            categorical_embeddings.append(np.zeros((dimpool)))\n",
    "    elements.append(np.array(categorical_embeddings))\n",
    "#elements \n",
    "reshaped_x = (np.reshape(elements,(len(elements),len(cat_cols)*dimpool)))   \n",
    "# Get the numerical columns\n",
    "numerical_cols = np.where(X_train.dtypes!=\"object\")[0]\n",
    "my_data = pd.concat([X_train.iloc[:, numerical_cols].reset_index(drop=True), pd.DataFrame(reshaped_x)], axis=1)\n",
    "# due to the new index of my_data, we have to change the index of y_train\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "#%% apply it on the X_test dataset \n",
    "elements_test = []\n",
    "for row in tqdm(X_test.iterrows()):\n",
    "    categorical_embeddings = []\n",
    "    for i in cat_cols:\n",
    "        #print(i,row[1][i])\n",
    "        try:\n",
    "            categorical_embeddings.append(model.wv[row[1][i]])\n",
    "        except:\n",
    "            categorical_embeddings.append(np.zeros((dimpool)))\n",
    "    elements_test .append(np.array(categorical_embeddings))\n",
    "\n",
    "#elements \n",
    "reshaped_x_test = (np.reshape(elements_test ,(len(elements_test ),len(cat_cols)*dimpool)))   \n",
    "# Get the numerical columns\n",
    "my_test_data = pd.concat([X_test.iloc[:, numerical_cols].reset_index(drop=True), pd.DataFrame(reshaped_x_test)], axis=1)\n",
    "# due to the new index of my_data, we have to change the index of y_train\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(my_data,y_train,test_size=0.2, random_state=1500)\n",
    "stc = StandardScaler()\n",
    "X_scaled = stc.fit_transform(X_train2.values)\n",
    "\n",
    "\n",
    "seed=42\n",
    "models = [\n",
    "    ('LR', LogisticRegression(solver='lbfgs', random_state=seed, max_iter=1000)),\n",
    "    ('DT', DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=seed)),\n",
    "    ('RF', RandomForestClassifier(n_estimators=200, max_depth=5, random_state=seed, min_samples_leaf=3)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=3)),\n",
    "    ('XGB', XGBClassifier(eval_metric='logloss')),\n",
    "    ('SVM', SVC(gamma='scale', random_state=seed, probability=True)),\n",
    "    ('MLP', KerasClassifier(build_fn=create_network, number_of_features=my_data.shape[1], epochs=100, batch_size=100, verbose=0))\n",
    "]\n",
    "\n",
    "for name, classifier in models:\n",
    "    start_time = time.time()\n",
    "    classifier.fit(X_scaled, y_train2)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(f\"Classifier: {name}\")\n",
    "    print(f\"Computation Time: {elapsed_time} seconds\")      \n",
    "#     y_pred = classifier.predict(stc.transform(X_test2.values))\n",
    "#     y_pred_prob = classifier.predict_proba(stc.transform(X_test2.values))\n",
    "\n",
    "#     print(confusion_matrix(y_test2,y_pred))\n",
    "#     print(classification_report(y_test2,y_pred, digits=4))\n",
    "#     print(roc_auc_score(y_test2,y_pred_prob[:,1]))\n",
    "    \n",
    "    y_pred_test = classifier.predict(stc.transform(my_test_data.values))\n",
    "    y_pred_prob_test = classifier.predict_proba(stc.transform(my_test_data.values))\n",
    "\n",
    "    print(confusion_matrix(y_test,y_pred_test))\n",
    "    print(classification_report(y_test,y_pred_test, digits=4))\n",
    "\n",
    "    print(roc_auc_score(y_test,y_pred_prob_test[:,1]))\n",
    "    print()\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
